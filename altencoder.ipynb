{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset members have the following lengths: {1158, 294} (expected 294)\n",
      "Has 1 corrupt value(s), 601 values, 601 keys\n",
      "dataset size: (601, 294)\n",
      "partitioning as train = (500, 294), test = (101, 294)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib\n",
    "import zipfile\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "SW_DATASET_URL = 'https://github.com/SeijiEmery/shape-net-data/raw/master/car-vertex-data/shrinkwrap-data.json.zip'\n",
    "with urllib.request.urlopen(SW_DATASET_URL) as f:\n",
    "    with zipfile.ZipFile(io.BytesIO(f.read()), 'r') as zf:\n",
    "        with zf.open('shrinkwrap-data.json', 'r') as f:\n",
    "            sw_dataset = json.loads(f.read())\n",
    "            \n",
    "sw_keys = list(sw_dataset.keys())\n",
    "sw_dimensions = set(map(len, sw_dataset.values()))\n",
    "print(\"dataset members have the following lengths: %s (expected 294)\"%(sw_dimensions,))\n",
    "\n",
    "sw_keys = [ key for key in sw_dataset.keys() if len(sw_dataset[key]) == 294 ]\n",
    "non_corrupt_values = [ x for x in sw_dataset.values() if len(x) == 294 ]\n",
    "corrupt_values = [ x for x in sw_dataset.values() if len(x) != 294 ]\n",
    "print(\"Has %s corrupt value(s), %s values, %s keys\"%(\n",
    "    len(corrupt_values), len(non_corrupt_values), len(sw_keys)))\n",
    "\n",
    "sw_data = np.array(non_corrupt_values)\n",
    "print(\"dataset size: %s\"%(sw_data.shape,))\n",
    "\n",
    "x_train, x_test = np.split(sw_data, [500], 0)\n",
    "print(\"partitioning as train = %s, test = %s\"%(x_train.shape, x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 80)                23600     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                810       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 80)                880       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 294)               23814     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 294)               0         \n",
      "=================================================================\n",
      "Total params: 49,104\n",
      "Trainable params: 49,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.models import Model, load_model\n",
    "import os\n",
    "\n",
    "INPUT_DIM = 294\n",
    "HIDDEN_DIM = 80\n",
    "LATENT_DIM = 10\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(HIDDEN_DIM, input_shape=(INPUT_DIM,)),\n",
    "    Activation('relu'),\n",
    "    Dense(LATENT_DIM),\n",
    "    Activation('relu'),\n",
    "    Dense(HIDDEN_DIM),\n",
    "    Activation('relu'),\n",
    "    Dense(INPUT_DIM),\n",
    "    Activation('linear')\n",
    "])\n",
    "trained_epochs = 0\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "def train (epochs, batch_size=50):\n",
    "    global model, trained_epochs\n",
    "    print(\"Training: epoch %s -> %s\"%(trained_epochs, epochs + trained_epochs))\n",
    "    model.fit(x_train, x_train,\n",
    "             epochs=epochs,\n",
    "             batch_size=batch_size)\n",
    "    trained_epochs += epochs\n",
    "    if not os.path.exists('altmodel'):\n",
    "        os.makedirs('altmodel')\n",
    "    path = os.path.join('altmodel', 'altmodel-epoch-%s.h5'%(trained_epochs,))\n",
    "    print(\"Saving snapshot as %s\"%path)\n",
    "    model.save(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 210 -> 220\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 48us/step - loss: 9.5483e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 57us/step - loss: 9.5620e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 64us/step - loss: 9.5454e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 50us/step - loss: 9.5621e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 60us/step - loss: 9.5354e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 68us/step - loss: 9.5418e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 70us/step - loss: 9.5680e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 73us/step - loss: 9.5606e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 67us/step - loss: 9.5411e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 72us/step - loss: 9.5281e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-220.h5\n",
      "Training: epoch 220 -> 230\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 80us/step - loss: 9.5081e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 83us/step - loss: 9.5267e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 88us/step - loss: 9.5183e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 86us/step - loss: 9.5128e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 78us/step - loss: 9.4978e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 83us/step - loss: 9.5055e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 80us/step - loss: 9.4649e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 83us/step - loss: 9.4785e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 92us/step - loss: 9.4856e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 97us/step - loss: 9.4957e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-230.h5\n",
      "Training: epoch 230 -> 240\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 96us/step - loss: 9.4818e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 91us/step - loss: 9.4760e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 90us/step - loss: 9.4544e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 73us/step - loss: 9.4664e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 74us/step - loss: 9.4579e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 78us/step - loss: 9.4745e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 77us/step - loss: 9.4658e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 85us/step - loss: 9.4577e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 104us/step - loss: 9.4930e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 82us/step - loss: 9.5044e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-240.h5\n",
      "Training: epoch 240 -> 250\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 75us/step - loss: 9.4672e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 66us/step - loss: 9.4309e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 87us/step - loss: 9.4162e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 72us/step - loss: 9.4020e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 76us/step - loss: 9.3995e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 71us/step - loss: 9.4364e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 77us/step - loss: 9.4135e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 83us/step - loss: 9.4023e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 70us/step - loss: 9.3809e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 85us/step - loss: 9.4332e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-250.h5\n",
      "Training: epoch 250 -> 260\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 77us/step - loss: 9.4141e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 94us/step - loss: 9.3946e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 88us/step - loss: 9.4175e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 80us/step - loss: 9.4145e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 81us/step - loss: 9.4006e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 77us/step - loss: 9.3938e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 72us/step - loss: 9.4386e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 66us/step - loss: 9.4216e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 79us/step - loss: 9.3954e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 79us/step - loss: 9.3998e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-260.h5\n",
      "Training: epoch 260 -> 270\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 68us/step - loss: 9.4371e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 80us/step - loss: 9.4169e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 78us/step - loss: 9.3778e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 75us/step - loss: 9.3584e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 77us/step - loss: 9.3632e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 75us/step - loss: 9.3696e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 74us/step - loss: 9.3536e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 80us/step - loss: 9.3359e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 63us/step - loss: 9.3844e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 58us/step - loss: 9.3466e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-270.h5\n",
      "Training: epoch 270 -> 280\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 57us/step - loss: 9.3867e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 71us/step - loss: 9.3262e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 69us/step - loss: 9.3457e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 72us/step - loss: 9.3726e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 70us/step - loss: 9.3486e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 65us/step - loss: 9.3121e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 72us/step - loss: 9.3181e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 76us/step - loss: 9.2971e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 83us/step - loss: 9.3203e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 82us/step - loss: 9.3697e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-280.h5\n",
      "Training: epoch 280 -> 290\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 71us/step - loss: 9.4020e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 77us/step - loss: 9.3329e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 77us/step - loss: 9.3185e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 61us/step - loss: 9.3245e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 68us/step - loss: 9.3168e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 68us/step - loss: 9.3211e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 62us/step - loss: 9.3065e-04\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 0s 65us/step - loss: 9.2937e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 65us/step - loss: 9.2865e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 72us/step - loss: 9.2802e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-290.h5\n",
      "Training: epoch 290 -> 300\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 0s 79us/step - loss: 9.2979e-04\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 0s 78us/step - loss: 9.2615e-04\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 0s 55us/step - loss: 9.2598e-04\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 0s 55us/step - loss: 9.3050e-04\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 0s 61us/step - loss: 9.3072e-04\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 0s 59us/step - loss: 9.3094e-04\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 0s 56us/step - loss: 9.3169e-04\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 52us/step - loss: 9.3347e-04\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 0s 56us/step - loss: 9.2886e-04\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 0s 49us/step - loss: 9.2795e-04\n",
      "Saving snapshot as altmodel/altmodel-epoch-300.h5\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading altmodel/altmodel-epoch-200.h5\n",
      "Generating prediction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((500, 294), (101, 294))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model_from_epoch (epoch):\n",
    "    path = os.path.join('altmodel', 'altmodel-epoch-%s.h5'%(epoch,))\n",
    "    print(\"Loading %s\"%path)\n",
    "    return load_model(path)\n",
    "\n",
    "def predict (epoch):\n",
    "    model = load_model_from_epoch(epoch)\n",
    "    print(\"Generating prediction\")\n",
    "    return map(model.predict, (x_train, x_test))\n",
    "y_train, y_test = predict(200)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# car-params/02958343-80b5df2ecf1051fd61c6c61410fc904b.json\n",
      "v -0.191121 0.0 0.0\n",
      "v -0.191121 0.006916 0.368814\n",
      "v -0.16805 0.146578 -0.0\n",
      "v -0.191121 0.005511 -0.363999\n",
      "v -0.178162 -0.0654 0.0\n",
      "v -0.184984 -0.082123 0.304755\n",
      "v -0.184742 0.092359 0.384368\n",
      "v -0.185406 0.081136 -0.326389\n",
      "v -0.184984 -0.082123 -0.297115\n",
      "v -0.0 0.031295 -0.41837\n",
      "v -0.191121 0.031295 -0.41837\n",
      "v -0.0 0.067398 -0.414911\n",
      "v 0.182494 0.031295 -0.41837\n",
      "v 0.0 -0.0654 -0.398229\n",
      "v -0.178164 -0.0654 -0.398229\n",
      "v -0.191121 0.067398 -0.414911\n",
      "v 0.182494 0.067398 -0.414911\n",
      "v 0.168471 -0.0654 -0.398229\n",
      "v 0.182494 0.0 0.0\n",
      "v 0.183668 -0.001021 -0.347688\n",
      "v 0.159942 0.146578 -0.0\n",
      "v 0.183668 0.001968 0.356492\n",
      "v 0.168471 -0.0654 0.0\n",
      "v 0.183339 -0.082123 -0.297674\n",
      "v 0.177305 0.081136 -0.326389\n",
      "v 0.175873 0.086533 0.323425\n",
      "v 0.183339 -0.082123 0.307491\n",
      "v -0.0 0.012726 0.47663\n",
      "v 0.182494 0.012726 0.47663\n",
      "v 0.0 0.082733 0.47663\n",
      "v -0.191121 0.012726 0.47663\n",
      "v 0.0 -0.067791 0.455745\n",
      "v 0.167312 -0.067791 0.455745\n",
      "v 0.176293 0.092359 0.47663\n",
      "v -0.184742 0.092359 0.47663\n",
      "v -0.175553 -0.067791 0.455745\n",
      "v 0.0 -0.0654 0.0\n",
      "v 0.137011 -0.088619 -0.298528\n",
      "v 0.168471 -0.0654 0.0\n",
      "v 0.137011 -0.088981 0.302829\n",
      "v -0.178162 -0.0654 0.0\n",
      "v -0.174924 -0.089214 -0.288892\n",
      "v 0.17328 -0.089214 -0.289451\n",
      "v 0.17328 -0.089214 0.299268\n",
      "v -0.174924 -0.089214 0.296532\n",
      "v 0.0 0.152111 -0.038878\n",
      "v 0.0 0.091514 -0.269046\n",
      "v -0.166583 0.152111 -0.038878\n",
      "v -0.016052 0.111852 0.27929\n",
      "v 0.158475 0.152111 -0.038878\n",
      "v 0.158475 0.152111 -0.038878\n",
      "v -0.166583 0.152111 -0.038878\n",
      "v -0.149987 0.114032 0.27613\n",
      "v 0.144788 0.10835 0.277834\n",
      "v -0.182476 -0.085863 -0.288892\n",
      "v -0.178162 -0.0654 0.0\n",
      "v -0.182476 -0.085863 0.296532\n",
      "v -0.175553 -0.067791 0.455745\n",
      "v -0.191121 0.012726 0.47663\n",
      "v -0.184742 0.092359 0.47663\n",
      "v -0.183643 0.087782 0.287956\n",
      "v -0.166583 0.152111 -0.038878\n",
      "v -0.185406 0.081136 -0.302083\n",
      "v -0.191121 0.067398 -0.414911\n",
      "v -0.191121 0.031295 -0.41837\n",
      "v -0.178164 -0.0654 -0.398229\n",
      "v 0.168471 -0.0654 -0.398229\n",
      "v 0.0 -0.0654 -0.398229\n",
      "v -0.178164 -0.0654 -0.398229\n",
      "v -0.191121 0.067398 -0.414911\n",
      "v -0.0 0.067398 -0.414911\n",
      "v 0.182494 0.067398 -0.414911\n",
      "v 0.182494 0.067398 -0.414911\n",
      "v 0.182494 0.031295 -0.41837\n",
      "v 0.168471 -0.0654 -0.398229\n",
      "v 0.180831 -0.085863 0.299268\n",
      "v 0.168471 -0.0654 0.0\n",
      "v 0.180831 -0.085863 -0.289451\n",
      "v 0.177305 0.081136 -0.302083\n",
      "v 0.158475 0.152111 -0.038878\n",
      "v 0.175543 0.087779 0.288047\n",
      "v 0.176293 0.092359 0.47663\n",
      "v 0.182494 0.012726 0.47663\n",
      "v 0.167312 -0.067791 0.455745\n",
      "v -0.175553 -0.067791 0.455745\n",
      "v 0.0 -0.067791 0.455745\n",
      "v 0.167312 -0.067791 0.455745\n",
      "v 0.176293 0.092359 0.47663\n",
      "v 0.0 0.082733 0.47663\n",
      "v -0.184742 0.092359 0.47663\n",
      "v -0.175553 -0.067791 0.455745\n",
      "v -0.184742 0.092359 0.47663\n",
      "v -0.178164 -0.0654 -0.398229\n",
      "v -0.191121 0.067398 -0.414911\n",
      "v 0.167312 -0.067791 0.455745\n",
      "v 0.176293 0.092359 0.47663\n",
      "v 0.168471 -0.0654 -0.398229\n",
      "v 0.182494 0.067398 -0.414911\n",
      "f 1//1 5//1 6//1 2//1\n",
      "f 2//2 6//2 58//2 59//2\n",
      "f 5//3 56//3 57//3 6//3\n",
      "f 6//4 57//4 91//4 58//4\n",
      "f 1//5 2//5 7//5 3//5\n",
      "f 3//6 7//6 61//6 62//6\n",
      "f 2//7 59//7 60//7 7//7\n",
      "f 7//8 60//8 92//8 61//8\n",
      "f 1//9 3//9 8//9 4//9\n",
      "f 4//10 8//10 64//10 65//10\n",
      "f 3//11 62//11 63//11 8//11\n",
      "f 8//12 63//12 94//12 64//12\n",
      "f 1//13 4//13 9//13 5//13\n",
      "f 5//14 9//14 55//14 56//14\n",
      "f 4//15 65//15 66//15 9//15\n",
      "f 9//16 66//16 93//16 55//16\n",
      "f 10//17 14//17 15//17 11//17\n",
      "f 11//18 15//18 66//18 65//18\n",
      "f 14//19 68//19 69//19 15//19\n",
      "f 15//20 69//20 93//20 66//20\n",
      "f 10//21 11//21 16//21 12//21\n",
      "f 12//22 16//22 70//22 71//22\n",
      "f 11//23 65//23 64//23 16//23\n",
      "f 16//24 64//24 94//24 70//24\n",
      "f 10//25 12//25 17//25 13//25\n",
      "f 13//26 17//26 73//26 74//26\n",
      "f 12//27 71//27 72//27 17//27\n",
      "f 17//28 72//28 98//28 73//28\n",
      "f 10//29 13//29 18//29 14//29\n",
      "f 14//30 18//30 67//30 68//30\n",
      "f 13//31 74//31 75//31 18//31\n",
      "f 18//32 75//32 97//32 67//32\n",
      "f 19//33 23//33 24//33 20//33\n",
      "f 20//34 24//34 75//34 74//34\n",
      "f 23//35 77//35 78//35 24//35\n",
      "f 24//36 78//36 97//36 75//36\n",
      "f 19//37 20//37 25//37 21//37\n",
      "f 21//38 25//38 79//38 80//38\n",
      "f 20//39 74//39 73//39 25//39\n",
      "f 25//40 73//40 98//40 79//40\n",
      "f 19//41 21//41 26//41 22//41\n",
      "f 22//42 26//42 82//42 83//42\n",
      "f 21//43 80//43 81//43 26//43\n",
      "f 26//44 81//44 96//44 82//44\n",
      "f 19//45 22//45 27//45 23//45\n",
      "f 23//46 27//46 76//46 77//46\n",
      "f 22//47 83//47 84//47 27//47\n",
      "f 27//48 84//48 95//48 76//48\n",
      "f 28//49 32//49 33//49 29//49\n",
      "f 29//50 33//50 84//50 83//50\n",
      "f 32//51 86//51 87//51 33//51\n",
      "f 33//52 87//52 95//52 84//52\n",
      "f 28//53 29//53 34//53 30//53\n",
      "f 30//54 34//54 88//54 89//54\n",
      "f 29//55 83//55 82//55 34//55\n",
      "f 34//56 82//56 96//56 88//56\n",
      "f 28//57 30//57 35//57 31//57\n",
      "f 31//58 35//58 60//58 59//58\n",
      "f 30//59 89//59 90//59 35//59\n",
      "f 35//60 90//60 92//60 60//60\n",
      "f 28//61 31//61 36//61 32//61\n",
      "f 32//62 36//62 85//62 86//62\n",
      "f 31//63 59//63 58//63 36//63\n",
      "f 36//64 58//64 91//64 85//64\n",
      "f 37//65 41//65 42//65 38//65\n",
      "f 38//66 42//66 69//66 68//66\n",
      "f 41//67 56//67 55//67 42//67\n",
      "f 42//68 55//68 93//68 69//68\n",
      "f 37//69 38//69 43//69 39//69\n",
      "f 39//70 43//70 78//70 77//70\n",
      "f 38//71 68//71 67//71 43//71\n",
      "f 43//72 67//72 97//72 78//72\n",
      "f 37//73 39//73 44//73 40//73\n",
      "f 40//74 44//74 87//74 86//74\n",
      "f 39//75 77//75 76//75 44//75\n",
      "f 44//76 76//76 95//76 87//76\n",
      "f 37//77 40//77 45//77 41//77\n",
      "f 41//78 45//78 57//78 56//78\n",
      "f 40//79 86//79 85//79 45//79\n",
      "f 45//80 85//80 91//80 57//80\n",
      "f 46//81 50//81 51//81 47//81\n",
      "f 47//82 51//82 72//82 71//82\n",
      "f 50//83 80//83 79//83 51//83\n",
      "f 51//84 79//84 98//84 72//84\n",
      "f 46//85 47//85 52//85 48//85\n",
      "f 48//86 52//86 63//86 62//86\n",
      "f 47//87 71//87 70//87 52//87\n",
      "f 52//88 70//88 94//88 63//88\n",
      "f 46//89 48//89 53//89 49//89\n",
      "f 49//90 53//90 90//90 89//90\n",
      "f 48//91 62//91 61//91 53//91\n",
      "f 53//92 61//92 92//92 90//92\n",
      "f 46//93 49//93 54//93 50//93\n",
      "f 50//94 54//94 81//94 80//94\n",
      "f 49//95 89//95 88//95 54//95\n",
      "f 54//96 88//96 96//96 81//96\n"
     ]
    }
   ],
   "source": [
    "OBJ_TEMPLATE = '''\n",
    "# {name}\n",
    "{vertices}\n",
    "f 1//1 5//1 6//1 2//1\n",
    "f 2//2 6//2 58//2 59//2\n",
    "f 5//3 56//3 57//3 6//3\n",
    "f 6//4 57//4 91//4 58//4\n",
    "f 1//5 2//5 7//5 3//5\n",
    "f 3//6 7//6 61//6 62//6\n",
    "f 2//7 59//7 60//7 7//7\n",
    "f 7//8 60//8 92//8 61//8\n",
    "f 1//9 3//9 8//9 4//9\n",
    "f 4//10 8//10 64//10 65//10\n",
    "f 3//11 62//11 63//11 8//11\n",
    "f 8//12 63//12 94//12 64//12\n",
    "f 1//13 4//13 9//13 5//13\n",
    "f 5//14 9//14 55//14 56//14\n",
    "f 4//15 65//15 66//15 9//15\n",
    "f 9//16 66//16 93//16 55//16\n",
    "f 10//17 14//17 15//17 11//17\n",
    "f 11//18 15//18 66//18 65//18\n",
    "f 14//19 68//19 69//19 15//19\n",
    "f 15//20 69//20 93//20 66//20\n",
    "f 10//21 11//21 16//21 12//21\n",
    "f 12//22 16//22 70//22 71//22\n",
    "f 11//23 65//23 64//23 16//23\n",
    "f 16//24 64//24 94//24 70//24\n",
    "f 10//25 12//25 17//25 13//25\n",
    "f 13//26 17//26 73//26 74//26\n",
    "f 12//27 71//27 72//27 17//27\n",
    "f 17//28 72//28 98//28 73//28\n",
    "f 10//29 13//29 18//29 14//29\n",
    "f 14//30 18//30 67//30 68//30\n",
    "f 13//31 74//31 75//31 18//31\n",
    "f 18//32 75//32 97//32 67//32\n",
    "f 19//33 23//33 24//33 20//33\n",
    "f 20//34 24//34 75//34 74//34\n",
    "f 23//35 77//35 78//35 24//35\n",
    "f 24//36 78//36 97//36 75//36\n",
    "f 19//37 20//37 25//37 21//37\n",
    "f 21//38 25//38 79//38 80//38\n",
    "f 20//39 74//39 73//39 25//39\n",
    "f 25//40 73//40 98//40 79//40\n",
    "f 19//41 21//41 26//41 22//41\n",
    "f 22//42 26//42 82//42 83//42\n",
    "f 21//43 80//43 81//43 26//43\n",
    "f 26//44 81//44 96//44 82//44\n",
    "f 19//45 22//45 27//45 23//45\n",
    "f 23//46 27//46 76//46 77//46\n",
    "f 22//47 83//47 84//47 27//47\n",
    "f 27//48 84//48 95//48 76//48\n",
    "f 28//49 32//49 33//49 29//49\n",
    "f 29//50 33//50 84//50 83//50\n",
    "f 32//51 86//51 87//51 33//51\n",
    "f 33//52 87//52 95//52 84//52\n",
    "f 28//53 29//53 34//53 30//53\n",
    "f 30//54 34//54 88//54 89//54\n",
    "f 29//55 83//55 82//55 34//55\n",
    "f 34//56 82//56 96//56 88//56\n",
    "f 28//57 30//57 35//57 31//57\n",
    "f 31//58 35//58 60//58 59//58\n",
    "f 30//59 89//59 90//59 35//59\n",
    "f 35//60 90//60 92//60 60//60\n",
    "f 28//61 31//61 36//61 32//61\n",
    "f 32//62 36//62 85//62 86//62\n",
    "f 31//63 59//63 58//63 36//63\n",
    "f 36//64 58//64 91//64 85//64\n",
    "f 37//65 41//65 42//65 38//65\n",
    "f 38//66 42//66 69//66 68//66\n",
    "f 41//67 56//67 55//67 42//67\n",
    "f 42//68 55//68 93//68 69//68\n",
    "f 37//69 38//69 43//69 39//69\n",
    "f 39//70 43//70 78//70 77//70\n",
    "f 38//71 68//71 67//71 43//71\n",
    "f 43//72 67//72 97//72 78//72\n",
    "f 37//73 39//73 44//73 40//73\n",
    "f 40//74 44//74 87//74 86//74\n",
    "f 39//75 77//75 76//75 44//75\n",
    "f 44//76 76//76 95//76 87//76\n",
    "f 37//77 40//77 45//77 41//77\n",
    "f 41//78 45//78 57//78 56//78\n",
    "f 40//79 86//79 85//79 45//79\n",
    "f 45//80 85//80 91//80 57//80\n",
    "f 46//81 50//81 51//81 47//81\n",
    "f 47//82 51//82 72//82 71//82\n",
    "f 50//83 80//83 79//83 51//83\n",
    "f 51//84 79//84 98//84 72//84\n",
    "f 46//85 47//85 52//85 48//85\n",
    "f 48//86 52//86 63//86 62//86\n",
    "f 47//87 71//87 70//87 52//87\n",
    "f 52//88 70//88 94//88 63//88\n",
    "f 46//89 48//89 53//89 49//89\n",
    "f 49//90 53//90 90//90 89//90\n",
    "f 48//91 62//91 61//91 53//91\n",
    "f 53//92 61//92 92//92 90//92\n",
    "f 46//93 49//93 54//93 50//93\n",
    "f 50//94 54//94 81//94 80//94\n",
    "f 49//95 89//95 88//95 54//95\n",
    "f 54//96 88//96 96//96 81//96\n",
    "'''.strip()\n",
    "\n",
    "def make_obj (verts, name):\n",
    "    return OBJ_TEMPLATE.format(name=name, vertices='\\n'.join([\n",
    "        'v %s %s %s'%(verts[i], verts[i+1], verts[i+2])\n",
    "        for i in range(0, len(verts) - 2, 3)\n",
    "    ]))\n",
    "print(make_obj(x_train[12], sw_keys[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading altmodel/altmodel-epoch-300.h5\n",
      "predicting for epoch 300\n",
      "writing altencoder-objs/300/input/train-02958343-835d8e35119c714bfdb3ae79bdfd4e2b-input.obj\n",
      "writing altencoder-objs/300/output/train-02958343-2ab27beb8a37ba37ac00fd1150223027-output.obj\n",
      "writing altencoder-objs/300/output/test-02958343-e27019b4147f868dbda733a39f84326d-output.obj\n",
      "writing altencoder-objs/300/input/train-02958343-2ab27beb8a37ba37ac00fd1150223027-input.obj\n",
      "writing altencoder-objs/300/input/test-02958343-e27019b4147f868dbda733a39f84326d-input.obj\n",
      "writing altencoder-objs/300/input/test-02958343-928a5f0e95e7aae5780bcccc86c008c3-input.obj\n",
      "writing altencoder-objs/300/output/train-02958343-835d8e35119c714bfdb3ae79bdfd4e2b-output.obj\n",
      "writing altencoder-objs/300/input/train-02958343-d07c8208ad9dc35780e74058ee862f05-input.obj\n",
      "writing altencoder-objs/300/output/test-02958343-928a5f0e95e7aae5780bcccc86c008c3-output.obj\n",
      "writing altencoder-objs/300/output/train-02958343-d07c8208ad9dc35780e74058ee862f05-output.obj\n",
      "writing altencoder-objs/300/input/test-02958343-6976cc9f5982474e9aae70753d127b0b-input.obj\n",
      "writing altencoder-objs/300/output/test-02958343-6976cc9f5982474e9aae70753d127b0b-output.obj\n",
      "writing altencoder-objs/300/input/train-02958343-625861912ac0d62651a95aaa6caba1d3-input.obj\n",
      "writing altencoder-objs/300/input/test-02958343-a1b7a3cc11b25b3a82c0a051a54c0e33-input.obj\n",
      "writing altencoder-objs/300/output/train-02958343-625861912ac0d62651a95aaa6caba1d3-output.obj\n",
      "writing altencoder-objs/300/output/test-02958343-a1b7a3cc11b25b3a82c0a051a54c0e33-output.obj\n",
      "writing altencoder-objs/300/output/train-02958343-244a8476648bd073834daea73aa18748-output.obj\n",
      "writing altencoder-objs/300/input/train-02958343-244a8476648bd073834daea73aa18748-input.obj\n",
      "writing altencoder-objs/300/input/test-02958343-96905400f6122662473f10e6caaeca56-input.obj\n",
      "writing altencoder-objs/300/input/train-02958343-e213d976734431773a3afd30f2e86bd7-input.obj\n",
      "writing altencoder-objs/300/output/test-02958343-96905400f6122662473f10e6caaeca56-output.obj\n",
      "writing altencoder-objs/300/output/train-02958343-e213d976734431773a3afd30f2e86bd7-output.obj\n",
      "writing altencoder-objs/300/input/test-02958343-17600a4881b1ff6a5c14de0cb75f4d3e-input.obj\n",
      "writing altencoder-objs/300/output/test-02958343-17600a4881b1ff6a5c14de0cb75f4d3e-output.obj\n",
      "writing altencoder-objs/300/input/test-02958343-7478c015f1814c0728ccbb4eb8965b05-input.obj\n",
      "writing altencoder-objs/300/input/train-02958343-c23a65ae787245248580e2c19c4f9aaf-input.obj\n",
      "writing altencoder-objs/300/output/train-02958343-c23a65ae787245248580e2c19c4f9aaf-output.obj\n",
      "writing altencoder-objs/300/output/test-02958343-7478c015f1814c0728ccbb4eb8965b05-output.obj\n",
      "writing altencoder-objs/300/input/train-02958343-936702a0367e95185b03bc28c642bc-input.obj\n",
      "writing altencoder-objs/300/output/train-02958343-936702a0367e95185b03bc28c642bc-output.obj\n",
      "writing altencoder-objs/300/input/train-02958343-d0e68e14e8012b497d6d702dc488f23a-input.obj\n",
      "writing altencoder-objs/300/output/test-02958343-fe1ec9b9ff75e947d56a18f240de5e54-output.obj\n",
      "writing altencoder-objs/300/input/test-02958343-fe1ec9b9ff75e947d56a18f240de5e54-input.obj\n",
      "writing altencoder-objs/300/output/train-02958343-d0e68e14e8012b497d6d702dc488f23a-output.obj\n",
      "writing altencoder-objs/300/output/test-02958343-14121abc9e2a868e52ab7aae4be20d81-output.obj\n",
      "writing altencoder-objs/300/input/train-02958343-167ec61fc29df46460593c98e3e63028-input.obj\n",
      "writing altencoder-objs/300/input/test-02958343-14121abc9e2a868e52ab7aae4be20d81-input.obj\n",
      "writing altencoder-objs/300/input/test-02958343-b5367bf3cb07b8ea593fee7ac9114e04-input.obj\n",
      "writing altencoder-objs/300/output/train-02958343-167ec61fc29df46460593c98e3e63028-output.obj\n",
      "writing altencoder-objs/300/output/test-02958343-b5367bf3cb07b8ea593fee7ac9114e04-output.obj\n",
      "generating write-obj jobs for epoch 300\n",
      "dispatching write-obj jobs for epoch 300\n",
      "done processing models for epoch 300\n",
      "Loading altmodel/altmodel-epoch-200.h5\n",
      "predicting for epoch 200\n",
      "writing altencoder-objs/200/output/train-02958343-2ab27beb8a37ba37ac00fd1150223027-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-2ab27beb8a37ba37ac00fd1150223027-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-e27019b4147f868dbda733a39f84326d-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-e27019b4147f868dbda733a39f84326d-input.obj\n",
      "writing altencoder-objs/200/input/train-02958343-835d8e35119c714bfdb3ae79bdfd4e2b-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-835d8e35119c714bfdb3ae79bdfd4e2b-output.obj\n",
      "writing altencoder-objs/200/output/test-02958343-928a5f0e95e7aae5780bcccc86c008c3-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-928a5f0e95e7aae5780bcccc86c008c3-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-d07c8208ad9dc35780e74058ee862f05-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-d07c8208ad9dc35780e74058ee862f05-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-6976cc9f5982474e9aae70753d127b0b-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-6976cc9f5982474e9aae70753d127b0b-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-625861912ac0d62651a95aaa6caba1d3-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-625861912ac0d62651a95aaa6caba1d3-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-244a8476648bd073834daea73aa18748-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-a1b7a3cc11b25b3a82c0a051a54c0e33-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-a1b7a3cc11b25b3a82c0a051a54c0e33-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-244a8476648bd073834daea73aa18748-input.obj\n",
      "writing altencoder-objs/200/input/test-02958343-96905400f6122662473f10e6caaeca56-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-96905400f6122662473f10e6caaeca56-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-17600a4881b1ff6a5c14de0cb75f4d3e-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-e213d976734431773a3afd30f2e86bd7-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-e213d976734431773a3afd30f2e86bd7-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-17600a4881b1ff6a5c14de0cb75f4d3e-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-c23a65ae787245248580e2c19c4f9aaf-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-c23a65ae787245248580e2c19c4f9aaf-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-7478c015f1814c0728ccbb4eb8965b05-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-7478c015f1814c0728ccbb4eb8965b05-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-936702a0367e95185b03bc28c642bc-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-936702a0367e95185b03bc28c642bc-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-fe1ec9b9ff75e947d56a18f240de5e54-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-fe1ec9b9ff75e947d56a18f240de5e54-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-d0e68e14e8012b497d6d702dc488f23a-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-d0e68e14e8012b497d6d702dc488f23a-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-14121abc9e2a868e52ab7aae4be20d81-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-14121abc9e2a868e52ab7aae4be20d81-output.obj\n",
      "writing altencoder-objs/200/input/test-02958343-b5367bf3cb07b8ea593fee7ac9114e04-input.obj\n",
      "writing altencoder-objs/200/output/train-02958343-167ec61fc29df46460593c98e3e63028-output.obj\n",
      "writing altencoder-objs/200/input/train-02958343-167ec61fc29df46460593c98e3e63028-input.obj\n",
      "writing altencoder-objs/200/output/test-02958343-b5367bf3cb07b8ea593fee7ac9114e04-output.obj\n",
      "generating write-obj jobs for epoch 200\n",
      "dispatching write-obj jobs for epoch 200\n",
      "done processing models for epoch 200\n",
      "Loading altmodel/altmodel-epoch-100.h5\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "pool = mp.Pool(12)\n",
    "\n",
    "def write_obj (args):\n",
    "    verts, path = args\n",
    "    print(\"writing %s\"%path)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(make_obj(verts, path))\n",
    "        \n",
    "def build_dirs (paths):\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "def export_model_objs (epoch, num_samples=10, output_dir='altencoder-objs', parallel=False):\n",
    "    model = load_model_from_epoch(epoch)\n",
    "    print(\"predicting for epoch %s\"%epoch)\n",
    "    y_train, y_test = map(model.predict, (x_train, x_test))\n",
    "    tasks = []\n",
    "    print(\"generating write-obj jobs for epoch %s\"%epoch)\n",
    "    for i in range(num_samples):\n",
    "        train_name = sw_keys[i].split('/')[1].split('.')[0]\n",
    "        test_name  = sw_keys[i+500].split('/')[1].split('.')[0]\n",
    "        \n",
    "        build_dirs([\n",
    "            os.path.join(output_dir, str(epoch), 'input'),\n",
    "            os.path.join(output_dir, str(epoch), 'output'),\n",
    "        ])\n",
    "        tasks.append((x_train[i], os.path.join(output_dir, str(epoch), 'input', 'train-%s-input.obj'%train_name)))\n",
    "        tasks.append((y_train[i], os.path.join(output_dir, str(epoch), 'output', 'train-%s-output.obj'%train_name)))\n",
    "        tasks.append((x_test[i], os.path.join(output_dir, str(epoch), 'input', 'test-%s-input.obj'%test_name)))\n",
    "        tasks.append((y_test[i], os.path.join(output_dir, str(epoch), 'output', 'test-%s-output.obj'%test_name)))\n",
    "    \n",
    "    print(\"dispatching write-obj jobs for epoch %s\"%epoch)\n",
    "    if parallel:\n",
    "        map(write_obj, tasks)\n",
    "    else:\n",
    "        pool.map(write_obj, tasks)\n",
    "    print(\"done processing models for epoch %s\"%epoch)\n",
    "\n",
    "export_model_objs(300)\n",
    "export_model_objs(200)\n",
    "export_model_objs(100)\n",
    "export_model_objs(50)\n",
    "export_model_objs(10)\n",
    "export_model_objs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = model.predict(x_train), model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.188041,  0.071757, -0.041727, -0.191469,  0.008376,  0.29926 ,\n",
       "       -0.186761,  0.076401, -0.039019, -0.191926,  0.010706, -0.317276,\n",
       "       -0.176773, -0.078195, -0.001716, -0.1874  , -0.107019,  0.266355,\n",
       "       -0.19355 ,  0.027168,  0.255872, -0.193178,  0.020274, -0.28742 ,\n",
       "       -0.1874  , -0.118797, -0.285778,  0.002345, -0.018081, -0.446181,\n",
       "       -0.139854, -0.006102, -0.403974,  0.001501,  0.021966, -0.43595 ,\n",
       "        0.144545, -0.006102, -0.403974,  0.002345, -0.066678, -0.445537,\n",
       "       -0.129798, -0.059359, -0.40573 , -0.140946,  0.028435, -0.388389,\n",
       "        0.145285,  0.028404, -0.388583,  0.132084, -0.059637, -0.406831,\n",
       "        0.192732,  0.071757, -0.041727,  0.196616,  0.010706, -0.317276,\n",
       "        0.191451,  0.076401, -0.039019,  0.19616 ,  0.008376,  0.29926 ,\n",
       "        0.181463, -0.078195, -0.001712,  0.19209 , -0.118797, -0.286042,\n",
       "        0.197869,  0.020274, -0.28742 ,  0.198237,  0.027142,  0.256007,\n",
       "        0.19209 , -0.107019,  0.26609 ,  0.001707,  0.020511,  0.437536,\n",
       "        0.137851,  0.004251,  0.408247,  0.002345,  0.072133,  0.431312,\n",
       "       -0.13686 ,  0.004259,  0.406332,  0.002345, -0.035484,  0.426571,\n",
       "        0.119201, -0.047896,  0.412154,  0.116169,  0.07029 ,  0.405576,\n",
       "       -0.111479,  0.07029 ,  0.405576, -0.11451 , -0.047896,  0.412154,\n",
       "       -0.      , -0.079406, -0.013809, -0.138883, -0.123187, -0.28625 ,\n",
       "        0.178353, -0.080933, -0.00401 , -0.138883, -0.114234,  0.253518,\n",
       "       -0.173662, -0.080933, -0.004008, -0.183861, -0.123187, -0.28625 ,\n",
       "        0.188552, -0.123187, -0.286515,  0.188552, -0.114234,  0.253253,\n",
       "       -0.183861, -0.114234,  0.253518, -0.030519,  0.130658,  0.036033,\n",
       "        0.002345,  0.070903, -0.276893, -0.109772,  0.124245,  0.031355,\n",
       "        0.002345,  0.12185 ,  0.193754,  0.114445,  0.124264,  0.031634,\n",
       "        0.165479,  0.052789, -0.262263, -0.160789,  0.052796, -0.262173,\n",
       "       -0.101655,  0.116738,  0.190779,  0.106345,  0.116738,  0.190779,\n",
       "       -0.183861, -0.123187, -0.28625 , -0.175661, -0.080082, -0.00318 ,\n",
       "       -0.183861, -0.114234,  0.253518, -0.183861, -0.086014,  0.302415,\n",
       "       -0.153931,  0.004705,  0.389794, -0.141717,  0.056613,  0.387792,\n",
       "       -0.153769,  0.072766,  0.277219, -0.185846,  0.077788, -0.038483,\n",
       "       -0.160856,  0.053089, -0.25876 , -0.149822,  0.030687, -0.379899,\n",
       "       -0.187486, -0.024256, -0.347802, -0.183861, -0.094951, -0.335138,\n",
       "        0.188552, -0.114265, -0.31396 ,  0.002345, -0.071818, -0.43928 ,\n",
       "       -0.183861, -0.114265, -0.313695, -0.143117,  0.035175, -0.373659,\n",
       "        0.002345,  0.040783, -0.40102 ,  0.147808,  0.035175, -0.373659,\n",
       "        0.154513,  0.030687, -0.379899,  0.192177, -0.024256, -0.347802,\n",
       "        0.188552, -0.094951, -0.335403,  0.188552, -0.114234,  0.253253,\n",
       "        0.180351, -0.080082, -0.003179,  0.188552, -0.123187, -0.286515,\n",
       "        0.165547,  0.053089, -0.25876 ,  0.190536,  0.077788, -0.038483,\n",
       "        0.15843 ,  0.072773,  0.277584,  0.146408,  0.056613,  0.387792,\n",
       "        0.158622,  0.004705,  0.389794,  0.188552, -0.086014,  0.30215 ,\n",
       "       -0.183861, -0.105321,  0.280965, -0.01808 , -0.052722,  0.40967 ,\n",
       "        0.188552, -0.105321,  0.2807  ,  0.112311,  0.073068,  0.403818,\n",
       "        0.002345,  0.074961,  0.428017, -0.109975,  0.072869,  0.402065,\n",
       "       -0.183861, -0.105321,  0.280965, -0.13768 ,  0.068677,  0.380554,\n",
       "       -0.183861, -0.114265, -0.313695, -0.15657 ,  0.040311, -0.347674,\n",
       "        0.188552, -0.105321,  0.2807  ,  0.142371,  0.068677,  0.380554,\n",
       "        0.188552, -0.114265, -0.31396 ,  0.161261,  0.040311, -0.347674])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.90569937e-01,  2.16046330e-02, -2.42814086e-02, -1.83027416e-01,\n",
       "       -5.75248152e-04,  3.04515243e-01, -1.83217198e-01,  6.07237145e-02,\n",
       "       -3.82540114e-02, -1.83499277e-01, -1.13823311e-02, -3.08607608e-01,\n",
       "       -1.74083784e-01, -6.93430454e-02, -3.85557255e-03, -1.81659818e-01,\n",
       "       -9.26676616e-02,  2.62888491e-01, -1.77214965e-01,  5.37756905e-02,\n",
       "        2.89658010e-01, -1.78229228e-01,  3.21672484e-02, -2.69728482e-01,\n",
       "       -1.81090280e-01, -9.46186632e-02, -2.68231452e-01,  3.77157331e-03,\n",
       "       -2.22427957e-02, -4.29328889e-01, -1.35522544e-01, -2.63939966e-02,\n",
       "       -4.06195939e-01, -5.12617640e-04,  4.54471121e-03, -4.19039786e-01,\n",
       "        1.32203668e-01, -2.92866174e-02, -4.10046130e-01,  1.58825703e-03,\n",
       "       -6.69832081e-02, -4.21616077e-01, -1.27337530e-01, -6.67602643e-02,\n",
       "       -4.04059321e-01, -1.36281550e-01, -8.13290011e-04, -3.94701898e-01,\n",
       "        1.35797545e-01,  4.93134093e-03, -3.95223409e-01,  1.30829513e-01,\n",
       "       -6.47926629e-02, -4.07911420e-01,  1.89748287e-01,  2.24832334e-02,\n",
       "       -3.00936736e-02,  1.88733622e-01, -7.21360371e-03, -2.99545795e-01,\n",
       "        1.85802177e-01,  6.18798211e-02, -3.95606384e-02,  1.86052516e-01,\n",
       "       -3.69463116e-04,  3.05189848e-01,  1.76283240e-01, -7.31266961e-02,\n",
       "       -1.20681636e-02,  1.83994919e-01, -9.18290243e-02, -2.64804929e-01,\n",
       "        1.86849222e-01,  2.59705745e-02, -2.71114945e-01,  1.80007786e-01,\n",
       "        5.14383540e-02,  2.90307611e-01,  1.85560942e-01, -9.50875729e-02,\n",
       "        2.65156686e-01, -7.41815381e-03,  2.43389048e-04,  4.21209395e-01,\n",
       "        1.45730406e-01,  1.85029954e-03,  4.04973090e-01, -3.96796735e-03,\n",
       "        7.25987703e-02,  4.09256101e-01, -1.42065346e-01,  8.77492130e-04,\n",
       "        4.02380884e-01, -1.98469125e-03, -5.50434925e-02,  4.09896731e-01,\n",
       "        1.37411773e-01, -5.34951314e-02,  3.95802945e-01,  1.38727918e-01,\n",
       "        7.35930651e-02,  3.96024346e-01, -1.39644042e-01,  7.41339326e-02,\n",
       "        3.96465987e-01, -1.34514183e-01, -5.27277552e-02,  3.95895779e-01,\n",
       "        3.51861678e-03, -7.63533264e-02, -1.04392879e-02, -5.85044920e-03,\n",
       "       -9.68048424e-02, -2.73534536e-01,  1.70462057e-01, -8.03478509e-02,\n",
       "       -1.01215579e-02, -3.69772427e-02, -1.00301802e-01,  2.63442457e-01,\n",
       "       -1.67297080e-01, -8.05289745e-02, -1.34731457e-02, -1.72575191e-01,\n",
       "       -1.00086711e-01, -2.55825639e-01,  1.77416846e-01, -1.01507075e-01,\n",
       "       -2.55771995e-01,  1.78404436e-01, -1.00469656e-01,  2.53004283e-01,\n",
       "       -1.74947888e-01, -9.90119278e-02,  2.52596676e-01, -8.52947123e-04,\n",
       "        1.29328877e-01,  2.46557016e-02, -4.44662943e-03,  7.82495663e-02,\n",
       "       -1.85622320e-01, -1.01650946e-01,  1.20962918e-01,  3.17645520e-02,\n",
       "       -1.02294376e-03,  1.10878453e-01,  2.39073515e-01,  1.05021536e-01,\n",
       "        1.20950490e-01,  2.94912346e-02,  1.54059529e-01,  5.70200868e-02,\n",
       "       -2.17190444e-01, -1.50111794e-01,  5.95310219e-02, -2.20794991e-01,\n",
       "       -1.27015218e-01,  9.94460136e-02,  2.43606746e-01,  1.26001403e-01,\n",
       "        1.02971107e-01,  2.49794215e-01, -1.77249700e-01, -9.94428694e-02,\n",
       "       -2.52445877e-01, -1.72170684e-01, -8.16790909e-02, -1.84037648e-02,\n",
       "       -1.77945703e-01, -9.97208431e-02,  2.52078652e-01, -1.67956263e-01,\n",
       "       -5.90812080e-02,  3.60402286e-01, -1.66262642e-01,  2.39650346e-03,\n",
       "        3.87880951e-01, -1.56906605e-01,  6.67805374e-02,  3.81301671e-01,\n",
       "       -1.56067386e-01,  8.36220533e-02,  2.60273337e-01, -1.67365670e-01,\n",
       "        8.85880291e-02, -2.72452421e-02, -1.72042608e-01,  5.05104586e-02,\n",
       "       -2.24798173e-01, -1.63400248e-01,  1.47109013e-02, -3.57257277e-01,\n",
       "       -1.66656867e-01, -2.33047642e-02, -3.74819160e-01, -1.60807177e-01,\n",
       "       -6.66287318e-02, -3.74163747e-01,  1.46879509e-01, -7.57013932e-02,\n",
       "       -3.86226833e-01, -1.96072366e-03, -7.21489713e-02, -4.20378417e-01,\n",
       "       -1.42605141e-01, -7.58526176e-02, -3.86551350e-01, -1.52989015e-01,\n",
       "        3.16066109e-02, -3.40790302e-01, -1.09812617e-03,  2.71620583e-02,\n",
       "       -3.74692738e-01,  1.51901573e-01,  3.23369391e-02, -3.44762474e-01,\n",
       "        1.70170650e-01,  1.58670116e-02, -3.51478517e-01,  1.68208376e-01,\n",
       "       -2.43107397e-02, -3.76066238e-01,  1.65650174e-01, -6.70867786e-02,\n",
       "       -3.73083025e-01,  1.82813391e-01, -9.95176435e-02,  2.50503808e-01,\n",
       "        1.74037516e-01, -7.76710734e-02, -2.71955617e-02,  1.79404035e-01,\n",
       "       -9.82166678e-02, -2.55080789e-01,  1.72184825e-01,  4.88232188e-02,\n",
       "       -2.22907767e-01,  1.66584700e-01,  8.66082609e-02, -2.90511809e-02,\n",
       "        1.61403239e-01,  8.42851847e-02,  2.60974139e-01,  1.58239707e-01,\n",
       "        6.42204881e-02,  3.81781310e-01,  1.70276210e-01,  1.78979337e-03,\n",
       "        3.86558771e-01,  1.73086494e-01, -5.99440709e-02,  3.56690764e-01,\n",
       "       -1.48749903e-01, -7.22739995e-02,  3.59459281e-01, -5.00336662e-03,\n",
       "       -5.93420379e-02,  4.04736221e-01,  1.55092344e-01, -6.92075714e-02,\n",
       "        3.59939277e-01,  1.40829116e-01,  7.44962245e-02,  3.90784442e-01,\n",
       "        2.12564133e-03,  7.46225864e-02,  4.06410336e-01, -1.39903009e-01,\n",
       "        7.31967464e-02,  3.92510355e-01, -1.79628626e-01, -8.61624256e-02,\n",
       "        2.98011988e-01, -1.52217612e-01,  6.77168742e-02,  3.74174953e-01,\n",
       "       -1.69327363e-01, -8.55343267e-02, -3.31722170e-01, -1.66259930e-01,\n",
       "        3.78276631e-02, -3.23181629e-01,  1.79944590e-01, -8.45502764e-02,\n",
       "        2.97651500e-01,  1.56036928e-01,  6.96540028e-02,  3.74160737e-01,\n",
       "        1.74117118e-01, -8.36487114e-02, -3.30611706e-01,  1.68222353e-01,\n",
       "        3.90516929e-02, -3.23364794e-01], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.34488608e-02,  6.98919507e-01,  4.18088800e-01,  4.40885132e-02,\n",
       "        1.06867815e+00, -1.75607919e-02,  1.89750662e-02,  2.05197386e-01,\n",
       "        1.96055403e-02,  4.39061063e-02,  2.06317309e+00,  2.73212973e-02,\n",
       "        1.52128197e-02,  1.13203589e-01, -1.24683715e+00,  3.06306420e-02,\n",
       "        1.34100846e-01,  1.30146190e-02,  8.43969764e-02, -9.79376122e-01,\n",
       "       -1.32042623e-01,  7.73834098e-02, -5.86625650e-01,  6.15528433e-02,\n",
       "        3.36697950e-02,  2.03526493e-01,  6.13992277e-02, -6.08346825e-01,\n",
       "       -2.30175083e-01,  3.77696750e-02,  3.09712673e-02, -3.32546649e+00,\n",
       "       -5.50020196e-03,  1.34151742e+00,  7.93102467e-01,  3.87893431e-02,\n",
       "        8.53805503e-02, -3.79951121e+00, -1.50309931e-02,  3.22704891e-01,\n",
       "       -4.57734276e-03,  5.36900921e-02,  1.89561453e-02, -1.24686473e-01,\n",
       "        4.11771205e-03,  3.30938804e-02,  1.02860172e+00, -1.62540600e-02,\n",
       "        6.53023690e-02,  8.26385688e-01, -1.70887788e-02,  9.49764489e-03,\n",
       "       -8.64507413e-02, -2.65569701e-03,  1.54811489e-02,  6.86675399e-01,\n",
       "        2.78796136e-01,  4.00902143e-02,  1.67379074e+00,  5.58825920e-02,\n",
       "        2.95053220e-02,  1.90065299e-01, -1.38814006e-02,  5.15267328e-02,\n",
       "        1.04410973e+00, -1.98150369e-02,  2.85444398e-02,  6.48162152e-02,\n",
       "       -6.04916098e+00,  4.21421258e-02,  2.27008895e-01,  7.42445882e-02,\n",
       "        5.56922935e-02, -2.80979307e-01,  5.67290188e-02,  9.19566703e-02,\n",
       "       -8.95157098e-01, -1.33983099e-01,  3.39895794e-02,  1.11488867e-01,\n",
       "        3.50751135e-03,  5.34572572e+00,  9.88133731e-01,  3.73148840e-02,\n",
       "       -5.71588603e-02,  5.64737818e-01,  8.01943506e-03,  2.69209695e+00,\n",
       "       -6.45710369e-03,  5.11367626e-02, -3.80340950e-02,  7.93967568e-01,\n",
       "        9.72386203e-03,  1.84635021e+00, -5.51220056e-01,  3.90890816e-02,\n",
       "       -1.52773661e-01, -1.16901859e-01,  3.96721980e-02, -1.94190518e-01,\n",
       "       -4.69919639e-02,  2.35508355e-02, -2.52648855e-01, -5.46867633e-02,\n",
       "        2.24619135e-02, -1.74693763e-01, -1.00880139e-01,  3.94469564e-02,\n",
       "                   inf,  3.84438652e-02,  2.44022893e-01,  9.57874980e-01,\n",
       "        2.14163488e-01,  4.44208337e-02,  4.42433994e-02,  7.23004314e-03,\n",
       "       -1.52407927e+00,  7.33752564e-01,  1.21961919e-01, -3.91469510e-02,\n",
       "        3.66511951e-02,  4.99209799e-03, -2.36156329e+00,  6.13822906e-02,\n",
       "        1.87522130e-01,  1.06285978e-01,  5.90561428e-02,  1.75991987e-01,\n",
       "        1.07299811e-01,  5.38183869e-02,  1.20492530e-01,  9.82090191e-04,\n",
       "        4.84774495e-02,  1.33253429e-01,  3.63415478e-03,  9.72051931e-01,\n",
       "        1.01725364e-02,  3.15746632e-01,  2.89621724e+00, -1.03614322e-01,\n",
       "        3.29624369e-01,  7.39811077e-02,  2.64162115e-02, -1.30617763e-02,\n",
       "        1.43622335e+00,  9.00414226e-02, -2.33902345e-01,  8.23405448e-02,\n",
       "        2.66650821e-02,  6.77361519e-02,  6.90085793e-02, -8.01509177e-02,\n",
       "        1.71860139e-01,  6.64050745e-02, -1.27566896e-01,  1.57827119e-01,\n",
       "       -2.49473397e-01,  1.48126458e-01, -2.76905457e-01, -1.84836172e-01,\n",
       "        1.17929836e-01, -3.09338108e-01,  3.59581432e-02,  1.92748671e-01,\n",
       "        1.18093008e-01,  1.98696140e-02, -1.99431939e-02, -4.78734743e+00,\n",
       "        3.21726561e-02,  1.27047612e-01,  5.67749674e-03,  8.65041372e-02,\n",
       "        3.13121027e-01, -1.91747387e-01, -8.01114886e-02,  4.90647510e-01,\n",
       "        4.90784577e-03, -1.07182658e-01, -1.79597219e-01,  1.67366236e-02,\n",
       "       -1.49470067e-02, -1.49191288e-01,  6.11273493e-02,  9.94389430e-02,\n",
       "       -1.38839270e-01,  2.92018758e-01, -6.95442399e-02,  4.85701624e-02,\n",
       "        1.31248366e-01, -9.06291979e-02,  5.20614550e-01,  5.95993230e-02,\n",
       "        1.11097006e-01,  3.92165163e-02, -7.76797129e-02,  1.25387236e-01,\n",
       "        2.98282990e-01, -1.16446798e-01,  2.21013253e-01,  3.37492730e-01,\n",
       "       -2.30178471e-01,  1.83612949e+00, -4.60847224e-03,  4.30285541e-02,\n",
       "        2.24386135e-01,  3.36169276e-01, -2.32252189e-01, -6.89786327e-02,\n",
       "        1.01446742e-01,  8.79644233e-02,  1.46828408e+00,  3.33985772e-01,\n",
       "        6.56507455e-02, -2.76952056e-02,  8.06840334e-02,  7.73339481e-02,\n",
       "       -1.01335486e-01,  4.82940280e-01,  7.48106285e-02,  1.24721608e-01,\n",
       "       -2.25674930e-03, -8.12653110e-02,  1.21461591e-01,  2.93458957e-01,\n",
       "       -1.12342541e-01,  3.04351527e-02,  1.28826414e-01,  1.08555150e-02,\n",
       "        3.50066475e-02,  3.01057244e-02, -7.55475359e+00,  4.85169332e-02,\n",
       "        2.02702657e-01,  1.09712269e-01, -4.00963167e-02,  8.03515080e-02,\n",
       "        1.38554001e-01,  1.25704852e-01, -1.13388452e-01,  2.45090535e-01,\n",
       "       -1.87668925e-02, -1.58193076e-01,  5.98372421e-02, -8.08132571e-02,\n",
       "       -1.34377052e-01,  1.54997786e-02, -7.34715847e-02,  6.19597583e-01,\n",
       "        8.29984265e-03,  8.20224967e-02,  3.03089371e-01, -1.80508901e-01,\n",
       "        1.90965442e-01,  3.13774086e-01, -2.79373876e-01,  7.23265121e-01,\n",
       "       -1.25564999e-01,  1.20433011e-02,  1.77455855e-01,  3.42889154e-01,\n",
       "       -2.82291689e-01, -2.53920953e-01, -1.95465118e-02,  3.22758212e-02,\n",
       "        9.35431424e-02,  4.51452930e-03,  5.04808536e-02, -2.72134657e-01,\n",
       "       -4.49774814e-03,  2.37639324e-02,  2.30194249e-02,  1.81906499e-01,\n",
       "       -6.06729963e-02, -1.05589858e-01,  1.39803110e-02,  1.67625291e-02,\n",
       "        7.90468703e-02,  2.51438964e-01, -5.74671898e-02, -6.18888008e-02,\n",
       "        6.16044473e-02,  7.04463688e-02,  4.56500608e-02,  1.97213505e-01,\n",
       "       -6.03900944e-02, -9.59881457e-02, -1.42260551e-02,  1.67998844e-02,\n",
       "        7.65565037e-02,  2.67941089e-01, -5.30376665e-02, -4.31682357e-02,\n",
       "        3.12397873e-02,  6.99195388e-02])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = x_train[0] - y_train[0]\n",
    "delta / x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
